{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d6a796",
   "metadata": {},
   "source": [
    "# Stage 6 â€” Data Preprocessing Notebook\n",
    "\n",
    "This notebook loads a raw dataset from `data/raw/`, applies reusable cleaning functions from `src/cleaning.py`, compares original vs cleaned data, and saves the result to `data/processed/`.\n",
    "\n",
    "> Update file paths if running from a different working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Paths\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Project paths (adjust if needed)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "# Ensure paths exist (processed folder may be created on save)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add src to sys.path so we can import cleaning utilities if running in Colab or loose env\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from cleaning import fill_missing_median, drop_missing, normalize_data\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('DATA_RAW:', DATA_RAW)\n",
    "print('DATA_PROCESSED:', DATA_PROCESSED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71092300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a raw dataset\n",
    "# If multiple CSVs exist, pick one by name; here we try to auto-detect the first CSV.\n",
    "import glob\n",
    "\n",
    "raw_candidates = sorted(glob.glob(str(DATA_RAW / \"*.csv\")))\n",
    "if not raw_candidates:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_RAW}. Place your raw CSV there.\")\n",
    "raw_path = Path(raw_candidates[0])\n",
    "df_raw = pd.read_csv(raw_path)\n",
    "\n",
    "print(f\"Loaded: {raw_path.name} | shape={df_raw.shape}\")\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick profile of raw data\n",
    "summary = {\n",
    "    \"shape\": df_raw.shape,\n",
    "    \"dtypes\": df_raw.dtypes.astype(str).to_dict(),\n",
    "    \"missing_per_column\": df_raw.isna().sum().to_dict()\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning steps\n",
    "# 1) Fill numeric NaNs with median\n",
    "df1 = fill_missing_median(df_raw)\n",
    "\n",
    "# 2) Drop remaining rows with any missing values in a chosen subset (or all)\n",
    "#    Adjust 'subset' to critical columns only if appropriate\n",
    "df2 = drop_missing(df1, how=\"any\", subset=None)\n",
    "\n",
    "# 3) Normalize numeric columns (choose method=\"standard\" or \"minmax\")\n",
    "df_clean, scale_params = normalize_data(df2, columns=None, method=\"standard\")\n",
    "\n",
    "print(\"Scaling parameters (per column):\")\n",
    "scale_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs cleaned\n",
    "comparison = {\n",
    "    \"raw_shape\": df_raw.shape,\n",
    "    \"clean_shape\": df_clean.shape,\n",
    "    \"raw_missing_total\": int(df_raw.isna().sum().sum()),\n",
    "    \"clean_missing_total\": int(df_clean.isna().sum().sum()),\n",
    "}\n",
    "pd.DataFrame([comparison])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3091b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "clean_name = raw_path.stem + \"_cleaned.csv\"\n",
    "clean_path = DATA_PROCESSED / clean_name\n",
    "df_clean.to_csv(clean_path, index=False)\n",
    "print(f\"Saved cleaned dataset to: {clean_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f04c7",
   "metadata": {},
   "source": [
    "## Notes & Assumptions\n",
    "\n",
    "- **Missingness strategy**: First filled numeric columns with medians to preserve row count, then dropped any remaining rows with missing values as a conservative second pass. If target labels exist, consider avoiding leakage by fitting medians using *training data only*.\n",
    "- **Scaling choice**: Used z-score standardization. For bounded features or models that prefer [0, 1], use `method=\"minmax\"`.\n",
    "- **Columns affected**: By default, functions operate on numeric columns only. You can pass a subset list to target specific columns.\n",
    "- **Reproducibility**: `normalize_data` returns a dictionary of fitted parameters. Save these if you need to transform future/holdout data consistently.\n",
    "- **Tradeoffs**:\n",
    "  - Filling vs dropping: Filling keeps more data at the cost of injecting central tendency; dropping can bias the sample if missingness is not MCAR.\n",
    "  - Standardization vs Min-Max: Standardization is robust to outliers in *scale* but not in *fit*; Min-Max preserves shape but is sensitive to extremes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
